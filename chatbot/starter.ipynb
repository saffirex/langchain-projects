{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b46d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7906eaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b047e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72554a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--22d9373b-58d0-4f0c-86da-5cd3d6f036a7-0', usage_metadata={'input_tokens': 9, 'output_tokens': 3, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68eb590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello there! How can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--3b6aeaba-ffd3-4956-8ab2-490279175103-0' usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}}\n",
      "content='Hello! How can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--d96ad646-011b-476b-b9ee-b375bef1915a-0' usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}}\n",
      "content='Hello! How can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--6bc2d465-c0f2-47a0-8fb4-47cf3e91e19c-0' usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# all these are equivalent\n",
    "print(model.invoke(\"Hello\"))\n",
    "print(model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}]))\n",
    "print(model.invoke([HumanMessage(\"Hello\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a0a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_2 = [\n",
    "    SystemMessage(\"You are a professional explainer who can explain anything in 3 sentences\"),\n",
    "    HumanMessage(\"explain what postgres is\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ba4ff",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a0e91b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post|greSQL is a| powerful, open-source relational database management system (RDBMS). It uses SQL| to store and retrieve data, ensuring data integrity and reliability. Known for its extens|ibility and compliance with SQL standards, PostgreSQL is suitable for a wide range of applications, from small projects to enterprise-level systems.\n",
      "|"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages_2):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a55474",
   "metadata": {},
   "source": [
    "### Prompt Templating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30055a",
   "metadata": {},
   "source": [
    "We’re currently passing a list of messages to the language model. The list is typically built from user input and application logic. This logic formats the raw input—often by adding system messages or filling in templates—into a message list.\n",
    "LangChain’s prompt templates help automate this process. They take raw user input and output a prompt ready for the model.\n",
    "Let’s create a prompt template that takes two inputs:\n",
    "language: The target translation language\n",
    "text: The text to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe8a1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba992568",
   "metadata": {},
   "source": [
    "\n",
    "> **Note:** `ChatPromptTemplate` supports multiple message roles within a single template. In this example, we format the `language` parameter into the **system message**, and the `text` into a **user message**.\n",
    "\n",
    "The input to this prompt template is a dictionary. You can test the template directly to see what it generates:\n",
    "\n",
    "```python\n",
    "prompt = prompt_template.invoke({\n",
    "    \"language\": \"Italian\",\n",
    "    \"text\": \"hi!\"\n",
    "})\n",
    "\n",
    "prompt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2e63a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6618d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b0bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
